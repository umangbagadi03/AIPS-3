{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ocklgnhK3ZN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SYVXvvRK9hh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6GsUUQ_LC0M"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0MMXC41LIeg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install txtai[pipeline]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U-d1DRyLL7c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install Pillow==9.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1aFx9ATJvYI",
        "outputId": "c2f6560b-1673-4dc6-8cf8-a98fe26f1b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app1.py\n",
        "\n",
        "import streamlit as st\n",
        "from txtai.pipeline import Textractor\n",
        "from txtai.embeddings import Embeddings\n",
        "  #Web Scraping\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "# Create embeddings model, backed by sentence-transformers & transformers\n",
        "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n",
        "\n",
        "url = \"https://cdn.pixabay.com/photo/2022/02/25/09/23/background-7033808_1280.jpg\"\n",
        "\n",
        "st.title(\"AIP-SÂ³\")\n",
        "st.write(\"AI Powered Smart Search System\")\n",
        "st.image(url)\n",
        "\n",
        "st.markdown('_Welecome to Question Answering System ðŸ§  ðŸ¤–_')\n",
        "\n",
        "a = st.sidebar.radio(\"SELECT -\", ['PDF', 'Website'])\n",
        "\n",
        "def my_function_pdf():\n",
        "  textract = Textractor(sentences=True)\n",
        "\n",
        "  data_lines = []\n",
        "  for i in (locations_max):\n",
        "    lines = textract(i)\n",
        "    data_lines.append(lines)\n",
        "  total_lines = []\n",
        "  for i in data_lines:\n",
        "    total_lines += i\n",
        "  seq = embeddings.similarity(quer, total_lines)\n",
        "  three_most = seq[0:3]\n",
        "  indexes = []\n",
        "  for i in three_most:\n",
        "    indexes.append(i[0])\n",
        "  for j in indexes:\n",
        "    st.write(total_lines[j])\n",
        "\n",
        "## webscrap function\n",
        "def my_web():\n",
        "  textract = Textractor(sentences=True)\n",
        "  data_lines = []\n",
        "  total_lines = []\n",
        "  article_text = \" \"\n",
        "  for i in (locations_max):\n",
        "    #print(i)\n",
        "    scraped_data = urllib.request.urlopen(i)\n",
        "    article = scraped_data.read()\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    for p in paragraphs:\n",
        "      article_text += p.text\n",
        "    lines = textract(i)\n",
        "    data_lines.append(lines)\n",
        "  total_lines = []\n",
        "  for i in data_lines:\n",
        "    total_lines += i\n",
        "  seq = embeddings.similarity(quer, total_lines)\n",
        "  three_most = seq[0:3]\n",
        "  indexes = []\n",
        "  for i in three_most:\n",
        "    indexes.append(i[0])\n",
        "  for j in indexes:\n",
        "    st.write(total_lines[j])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "if a == 'PDF' :\n",
        "  number = st.number_input('Insert a number of files -',value =1, step =1)\n",
        "  st.write('Number of PDF files - ', number)\n",
        "  st.markdown(\"---\")\n",
        "  locations_max = []\n",
        "  for i in range (number) :\n",
        "    loc = st.text_input('Enter the PDF path :', placeholder = 'ex- /content/drive/MyDrive/', key = i)\n",
        "    locations_max.append(loc)\n",
        "\n",
        "  # for query\n",
        "  quer = st.text_input('ask me anything!', placeholder = 'ex - what is AI?')\n",
        "  st.write('Your query is - ', quer)\n",
        "\n",
        "  # for textraction\n",
        "  if st.button('Confirm!'):\n",
        "     st.write('Confirmed')\n",
        "     my_function_pdf()\n",
        "  else:\n",
        "     st.write('')\n",
        "## web\n",
        "else:\n",
        "  number = st.number_input('Insert a number of Links -',value =1, step =1)\n",
        "  st.write('Number of web pages - ', number)  \n",
        "  st.markdown(\"---\")\n",
        "  locations_max = []\n",
        "  for i in range (number) :\n",
        "    loc = st.text_input('Enter the URL :', placeholder = 'ex- https:\\\\', key = i)\n",
        "    locations_max.append(loc)\n",
        "\n",
        "  # for query\n",
        "  quer = st.text_input('ask me anything!', placeholder = 'ex - what is AI?')\n",
        "  st.write('Your query is - ', quer)\n",
        "  \n",
        "  if st.button('Confirm!'):\n",
        "     st.write('Confirmed')\n",
        "     my_web()\n",
        "  else:\n",
        "     st.write('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbQnLYTxKduH",
        "outputId": "8574a4ae-2c49-4ac0-82fa-9e37ea635b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-06-21 08:38:40.144 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.871s\n",
            "your url is: https://easy-donkeys-design-34-68-65-141.loca.lt\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.68.65.141:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2022-06-21 08:45:13.370 Loading faiss with AVX2 support.\n",
            "2022-06-21 08:45:13.370 Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "2022-06-21 08:45:13.370 Loading faiss.\n",
            "2022-06-21 08:45:13.388 Successfully loaded faiss.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2022-06-21 09:17:11,044 [ScriptRunner] [INFO ]  Retrieving https://www.geeksforgeeks.org/what-is-reinforcement-learning/ to /tmp/what-is-reinforcement-learning.\n",
            "2022-06-21 09:17:11.044 Retrieving https://www.geeksforgeeks.org/what-is-reinforcement-learning/ to /tmp/what-is-reinforcement-learning.\n",
            "2022-06-21 09:17:12,605 [ScriptRunner] [INFO ]  Retrieving https://mitpress.mit.edu/books/reinforcement-learning-second-edition to /tmp/books-reinforcement-learning-second-edition.\n",
            "2022-06-21 09:17:12.605 Retrieving https://mitpress.mit.edu/books/reinforcement-learning-second-edition to /tmp/books-reinforcement-learning-second-edition.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2022-06-21 09:26:18,088 [ScriptRunner] [INFO ]  Retrieving https://www.geeksforgeeks.org/what-is-reinforcement-learning/ to /tmp/what-is-reinforcement-learning.\n",
            "2022-06-21 09:26:18.088 Retrieving https://www.geeksforgeeks.org/what-is-reinforcement-learning/ to /tmp/what-is-reinforcement-learning.\n",
            "2022-06-21 09:26:18,611 [ScriptRunner] [INFO ]  Retrieving https://mitpress.mit.edu/books/reinforcement-learning-second-edition to /tmp/books-reinforcement-learning-second-edition.\n",
            "2022-06-21 09:26:18.611 Retrieving https://mitpress.mit.edu/books/reinforcement-learning-second-edition to /tmp/books-reinforcement-learning-second-edition.\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app1.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P73odfrMIzl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "AI Powerd Smart Search System - AIPS^3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}